import numpy as np


x=np.array([[3,3],[5, 1],[7, 4],[8, 0],[0, 1],[8, 8],[7, 2],[9, 9],[8, 6],[2, 8]])
y=np.array([15, 13, 16, 14, 12, 10, 16, 26, 18, 17])

# 梯度下降思想来实现 找出w1,w2 使得均方误差最小化(比较小)
# 1.初始化模型的参数(w:w1,w2)
w1=1
w2=5

# 2.损失函数
def loss(w1,w2):
    e=((3*w1+3*w2)-15)**2+((5*w1+1*w2)-13)**2+((7*w1+4*w2)-16)**2+((8*w1+0*w2)-14)**2+((0*w1+1*w2)-12)**2+((8*w1+8*w2)-10)**2+((7*w1+2*w2)-16)**2+((9*w1+9*w2)-26)**2+((8*w1+6*w2)-18)**2
    return e/10
# e=loss(w1,w2)
# print(e)

# 3.导函数
# 假设w1不变  那么w2该 增大还是减少才能让loss变小
# 因为我们只修改w2 所以把w1当前的值带入loss函数 得到一个只有一个未知数w2的函数(一元二次方程 抛物线)
#
def grad_w2(w1,w2):
    return 2*176*w2-954+402*w1
# w2=w2-0.01*grad_w2(w1,w2)
# 假设w2不变  那么w1该 增大还是减少才能让loss变小
# 因为我们只修改w1 所以把w2当前的值带入loss函数 得到一个只有一个未知数w1的函数(一元二次方程 抛物线)
def grad_w1(w1,w2):
    return 2*341*w1+402*w2-1512
# w1=w1-0.01*grad_w1(w1,w2)


for i in range(100):
    w1,w2=w1-0.01*grad_w1(w1,w2),w2-0.01*grad_w2(w1,w2)
